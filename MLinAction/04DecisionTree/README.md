决策树算法
==========
一、概述
-------
决策树既可以用于分类，也可以用于回归。但是本次实验我将讨论的是分类问题。
决策树是通过树形结构，不断进行if else判断来决定分类规则，其最大的有点是：直观。在本次实验中我将画出“决策树”，有了这样的树，即使人工判别也可以非常清晰的得到分类结果。

二、理论
-------
>熵（entropy）： 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。<br>
信息论（information theory）中的熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。<br>
信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益。

>优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。<br>
缺点：容易过拟合。

简单的说，就是，每经过一次if else判断，信息就会变得更加有序，也就越接近分类结果，直到最后的一次可以将所有信息进行分类为止。

三、案例
----------
本次实验的案例依然是用的第一次，knn算法中的数据集，性别分类问题。但是我决定不再手工写出决策树的所有算法实现了，而是调用sklearn库里面的内容，毕竟以后工作中这个库用的比较多。而且，库里面的内容肯定会比我们自己实现的更加高效，更加专业。
<br>
本次实验新增了对模型的评估方面的内容，通过sklearn模块，计算了准确率和召回率，并且打印出了评估报告。
<br>
感谢apacheCN社区提供的sklearn中文文档：[http://sklearn.apachecn.org/cn/0.19.0/index.html](http://sklearn.apachecn.org/cn/0.19.0/index.html) <br>
详细代码及注释请参考decisionTree.py文件。
